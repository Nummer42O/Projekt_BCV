A person operating a motor vehicle always needs to be aware of their surroundings and the environment. They know that the car they saw coming in the background is not gone just because the turning car in front of them is blocking it. This phenomena is called object permanency and is something the majority of humans learn growing up.
That same concept is not hard for a computer to grasp either but what might be is the preamble of \enquote{detecting the car in the background}. Robots rely heavily on images and depth information to orient themselves in their environments but this has some challenges, such as lighting conditions, camera resolution, framerate, etc.
There are some options to combat this, but they either come with their own trade offs or may not be applicable in all situations.

Changing to a camera with a thicker lens will improve the zoom, but since the image size stays the same the overall field of view (FOV) is decreased. This may affect close by detection and analysis.
As a fix for the above method or a standalone approach it might be useful to change recording resolution to something greater. But also this approach has drawbacks as it requires way more bandwidth and computing power to process those larger images.
Another option to combat the lens problem might be varifocal lenses but they add complexity, as well as cost and might not be easily applicable in stereo camera setups since both lenses would need to be well synchronized.

A comparatively low cost solution arises in image superresolution implemented as an AI supersampling model. Pairing this up with monocular depth estimation, a powerful methodology may be born. With this concept no extra camera hardware would be needed. This will be tested in this project in the context of robotics applications in particular.