\subsection{Peak Signal Noise Ratio}

The signal to noise ratio is a common tool to get a measurement of how much a signal is influenced by random noise. It is calculated by weighing the power of the signal itself against the power of the noise.
\begin{align}
    SNR = \frac{P_{\text{Signal}}}{P_{\text{Noise}}}
\end{align}
As images can be seen as discrete signals in two dimensions this concept can also be applied. To avoid evaluating every pixel individually the peak signal to noise ratio (PSNR) can be used which represents the maximum over the signal to noise ratio. In order to calculate it the mean squared error (MSE) over the difference between the image in question and the original image is needed. Typically the PSNR is calculated logarithmically to accommodate a wide range of MSEs.
\begin{align}
    PSNR &= 10 * \log_{10}(\frac{R^2}{MSE}) [\mathrm{dB}] \\
    \text{with: } & R = 255 \text{ …maximum fluctuation in the input image data type} \nonumber \\
    & MSE \text{ …mean squared error} \nonumber
\end{align}
As a general rule of thumb a higher PSNR means a better result and a better quality image.
A note for this implementation: When the MSE equates to zero $-1$ is returned as the PSNR would be $\infty$ which would make further calculations difficult.


\subsection{Monocular Depth Estimation} \label{mde_impl}

To lay a baseline for the accuracy of the GLPDepth model we compared it to a ground truth and calculated the MSE, PSNR in Dezibel and computation time. To do this we utilized svo files generated by a Stereolabs ZED2i stereo camera. Those files contain the video streams of both lenses, as well as of all other sensors. In our case the videostream had a resolution of 1920x1080 and was recorded with a framerate of 15fps.
The accompanying ZED SDK was used to calculate the depth images from that stereo camera setup as the ground truth. As will be discussed later, this is not necessarily optimal but sufficient.
The logged data gets temporarily stored in a numpy array and later written to an output csv file along with the frame index and time. The median and mean values over the aforementioned critical values get calculated in post, just before saving.


\subsection{super-resolution}

The implementation is kept simple. First the given raw-images get scaled down by $4$ using the resize method of OpenCV.

\begin{figure}[ht!]
    \begin{center}
        \includegraphics[scale=.44]{resources/qualitative_cmp_01.jpg}
        \caption{Comparison of the used super-resolution image to other models. \cite{ape_bib}} \label{ape}
    \end{center}
\end{figure}

Then the pretrained model gets loaded which can be obtained in the README of the ESRGAN \href{https://github.com/xinntao/ESRGAN}{repository} and yields results as shown in figure \ref*{ape}. As can be seen it compares quite well against other super-resolution models. In the repository are two models ready to use, the RRDB\textunderscore ESRGAN\textunderscore x4.pth and the RRDB\textunderscore PSNR\textunderscore x4.pth. We tested them both and stayed with the ESRGAN model. After loading the model with the parameters, the low resolution images get applied.

Lastly the PSNR is calculated over the raw images and the results after the process.

\subsection{Combination}

\begin{figure}[ht!]
    \begin{center}
        \includegraphics[scale=.5]{resources/general_plan.pdf}
        \caption{Flowchart of the final implementation.} \label{flowchart_general}
    \end{center}
\end{figure}

The actual implementation is very similar to the baseline implementation (Chapter \ref*{mde_impl}) in that there is a ground truth extracted from the given stereo information. But since the fine-grained information which would be the goal of the extraction with the super-resolution is not available via this method the effect is approximated by first downscaling the left camera image by a factor of $4$ using OpenCV and subsequent upscaling via the super-resolution model. This upscaled image is then used as an input for the monocular depth estimation model to produce a depth image which in turn is compared against the ground truth. Again MSE and PSNR are used to determine the quality of the generated prediction. The time it took to process the images in the neural networks is also logged. Those evaluations are then stored and later saved to a csv file along with the mean and median values of the processing time, MSE and PSNR for evaluation.

The max depth is taken from the parameters used for the svo recording, which in this case is a range from $0.3\mathrm{m}$ to $20.0\mathrm{m}$. The initialization values for the GLPDepth model are taken from the original implementations  \href{https://github.com/vinvino02/GLPDepth}{repository} to accommodate the respective pretrained weights. The latter can also be downloaded as stated in the repositories README. Only nyudepthv2\textunderscore swin\textunderscore large.ckpt and kitti\textunderscore swin\textunderscore large.ckpt are needed.

The two pretrained weights available to choose from differentiate in the dataset used for training. One was trained on the KITTI Eigen Split while the other one was trained on Nyu Depth V2. These datasets are different from each other in the sense that the former covers primarily the outdoors while the latter contains inside references.

All images are cut or resized to a size of 1216x352 which is rather arbitrary and thus just adopted from the original implementation. This size could be some other size that fulfills the rule of being divisible by 4 without a remainder in both directions. This constraint is necessary since the code does not explicitly check for mismatches between the ground truths size and the resulting depth estimations size resulting from a potential violation of this rule.