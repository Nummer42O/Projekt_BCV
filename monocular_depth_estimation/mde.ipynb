{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Depth Estimation with Masked Image Modeling\n",
    "\n",
    "This notebook will show monocular depth estimation for robotic applications.  \n",
    "The used model is taken from [this](https://github.com/SwinTransformer/MIM-Depth-Estimation/tree/main) repository for the paper [\"Revealing the Dark Secrets of Masked Image Modeling (Depth Estimation)\"](https://arxiv.org/abs/2205.13543)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#std libs\n",
    "from collections import OrderedDict\n",
    "import math\n",
    "\n",
    "#non std libs\n",
    "import torch\n",
    "from torchvision.transforms import ToTensor\n",
    "import cv2 as cv\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pyzed import sl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.model import GLPDepth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters\n",
    "\n",
    "Name           | Type   | Value\n",
    "---------------|--------|----------------------------------------------------------------------------------------------------------------------------------------------\n",
    "`SVO_FILEPATH` | `str`  | Path to the svo recording used for evaluation.\n",
    "`USE_NYU`      | `bool` | `True`: Use the weights pretrained on \"nyudepthv2\"<br>`False`: Use the weights pretrained on the kitti eigen split\n",
    "`CROP_TO_SIZE` | `bool` | `True`: Crop the image to size (1216, 352). This loses parts of the image completely.<br>`False`: Resize the image. This distortes the image.\n",
    "`OUTPUT_FILE`  | `str`  | Path to the output csv file for evaluation results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SVO_FILEPATH    = \"recording_rear.svo\";\n",
    "USE_NYU         = True;\n",
    "CROP_TO_SIZE    = False;\n",
    "OUTPUT_FILE     = \"output.csv\";"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_DEPTH = 20.0 #[m]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definition utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PSNR(ground_truth: np.ndarray, estimation: np.ndarray) -> float:\n",
    "    mse = np.nanmean((ground_truth - estimation) ** 2);\n",
    "    if (mse == 0): #no noise\n",
    "        return -1.0;\n",
    "\n",
    "    psnr = 20 * math.log10(255.0) - 10 * math.log10(mse);\n",
    "    return psnr;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_image(img: np.ndarray) -> np.ndarray:\n",
    "    h_im, w_im = img.shape[:2]\n",
    "\n",
    "    margin_top = int(h_im - 352)\n",
    "    margin_left = int((w_im - 1216) / 2)\n",
    "\n",
    "    sized_image = img[margin_top:  margin_top  + 352,\n",
    "                      margin_left: margin_left + 1216]\n",
    "\n",
    "    return sized_image;\n",
    "\n",
    "if CROP_TO_SIZE:\n",
    "    resize_image = crop_image;\n",
    "else:\n",
    "    resize_image = lambda img: cv.resize(img, (1216, 352));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialization of camera object for svo playback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "camera_init_parameters = sl.InitParameters();\n",
    "\n",
    "camera_init_parameters.svo_real_time_mode = False;\n",
    "camera_init_parameters.open_timeout_sec = 30;\n",
    "camera_init_parameters.coordinate_units = sl.UNIT.METER;\n",
    "\n",
    "camera_init_parameters.set_from_svo_file(SVO_FILEPATH);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "camera = sl.Camera();\n",
    "error_code = camera.open(camera_init_parameters);\n",
    "if (error_code != sl.ERROR_CODE.SUCCESS):\n",
    "    print(\"Failed to open Camera object:\", error_code);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nr_frames = camera.get_svo_number_of_frames();\n",
    "resolution = camera.get_camera_information().camera_configuration.camera_resolution;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "color_image = sl.Mat(resolution.width, resolution.height, sl.MAT_TYPE.U8_C3, sl.MEM.CPU);\n",
    "depth_image = sl.Mat(resolution.width, resolution.height, sl.MAT_TYPE.U8_C1, sl.MEM.CPU);\n",
    "# depth_image = sl.Mat(resolution.width, resolution.height, sl.MAT_TYPE.F32_C1, sl.MEM.CPU);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup of pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\");\n",
    "    device_prop = torch.cuda.get_device_properties(device);\n",
    "    print(f\"Using GPU: {device_prop.name} {round(device_prop.total_memory / 1024**3, 2)}GiB (CC: {device_prop.major}.{device_prop.minor})\");\n",
    "else:\n",
    "    device = torch.device(\"cpu\");\n",
    "    print(\"Using CPU.\");\n",
    "torch.set_default_device(device);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup of monocular depth estimation model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Storage:\n",
    "    #a minimal storage class to somewhat mimic the behaviour of argparse.ArgumentParser\n",
    "    def __init__(self, **kwargs):\n",
    "        self.__dict__.update(kwargs);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### KITTI parameters\n",
    "\n",
    "```bash\n",
    "python3 test.py \\\n",
    "--dataset kitti \\\n",
    "--kitti_crop garg_crop \\\n",
    "--data_path ../data/ \\\n",
    "--max_depth 80.0 \\\n",
    "--max_depth_eval 80.0 \\\n",
    "--backbone swin_large_v2 \\\n",
    "--depths 2 2 18 2 \\\n",
    "--num_filters 32 32 32 \\\n",
    "--deconv_kernels 2 2 2 \\\n",
    "--window_size 22 22 22 11 \\\n",
    "--pretrain_window_size 12 12 12 6 \\\n",
    "--use_shift True True False False \\\n",
    "--flip_test \\\n",
    "--shift_window_test \\\n",
    "--shift_size 16 \\\n",
    "--do_evaluate \\\n",
    "--ckpt_dir ckpt/kitti_swin_large.ckpt\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kitti_args = Storage(\n",
    "    #max_depth=80.0,\n",
    "    backbone=\"swin_large_v2\",\n",
    "    depths=[2, 2, 18, 2],\n",
    "    window_size=[22, 22, 22, 11],\n",
    "    pretrain_window_size=[12, 12, 12, 6],\n",
    "    drop_path_rate=0.3,\n",
    "    use_checkpoint=False,\n",
    "    use_shift=[True, True, False, False],\n",
    "    pretrained='',\n",
    "    num_deconv=3,\n",
    "    num_filters=[32, 32, 32],\n",
    "    deconv_kernels=[2, 2, 2],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### nyudepth parameters\n",
    "\n",
    "```bash\n",
    "python3 test.py \\\n",
    "--dataset nyudepthv2 \\\n",
    "--data_path ../data/ \\\n",
    "--max_depth 10.0 \\\n",
    "--max_depth_eval 10.0  \\\n",
    "--backbone swin_large_v2 \\\n",
    "--depths 2 2 18 2 \\\n",
    "--num_filters 32 32 32 \\\n",
    "--deconv_kernels 2 2 2 \\\n",
    "--window_size 30 30 30 15 \\\n",
    "--pretrain_window_size 12 12 12 6 \\\n",
    "--use_shift True True False False \\\n",
    "--flip_test \\\n",
    "--shift_window_test \\\n",
    "--shift_size 2 \\\n",
    "--do_evaluate \\\n",
    "--ckpt_dir ckpt/nyudepthv2_swin_large.ckpt\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nyudepthv2_args = Storage(\n",
    "    #max_depth=10.0,\n",
    "    backbone=\"swin_large_v2\",\n",
    "    depths=[2, 2, 18, 2],\n",
    "    window_size=[30, 30, 30, 15],\n",
    "    pretrain_window_size=[12, 12, 12, 6],\n",
    "    drop_path_rate=0.3, #\n",
    "    use_checkpoint=False, #\n",
    "    use_shift=[True, True, False, False],\n",
    "    pretrained='', #\n",
    "    num_deconv=3, #\n",
    "    num_filters=[32, 32, 32],\n",
    "    deconv_kernels=[2, 2, 2],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# as specified for the svo recording\n",
    "nyudepthv2_args.max_depth = kitti_args.max_depth = 20.0;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if USE_NYU:\n",
    "    print(\"Using weights pretrained on nyudepthv2.\")\n",
    "    model = GLPDepth(args=nyudepthv2_args);\n",
    "\n",
    "    model_weights: dict = torch.load(\"./checkpoints/nyudepthv2_swin_large.ckpt\", map_location=device);\n",
    "else:\n",
    "    print(\"Using weights pretrained on kitti.\")\n",
    "    model = GLPDepth(args=kitti_args);\n",
    "\n",
    "    model_weights: dict = torch.load(\"./checkpoints/kitti_swin_large.ckpt\", map_location=device);\n",
    "\n",
    "model = model.to(device);\n",
    "\n",
    "if 'module' in next(iter(model_weights.items()))[0]:\n",
    "    model_weight = OrderedDict((k[7:], v) for k, v in model_weights.items())\n",
    "\n",
    "model.load_state_dict(model_weights);\n",
    "model.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = [];\n",
    "\n",
    "while (True):\n",
    "    # get/go to the current frame\n",
    "    error_code = camera.grab();\n",
    "    if (error_code == sl.ERROR_CODE.END_OF_SVOFILE_REACHED):\n",
    "        print(\"Done\" + ' ' * 30)\n",
    "        break\n",
    "    elif (error_code != sl.ERROR_CODE.SUCCESS):\n",
    "        raise SystemExit(f\"Failed to grab frame: {error_code}\");\n",
    "\n",
    "    # retrieve current camera frame\n",
    "    error_code = camera.retrieve_image(color_image, sl.VIEW.LEFT, sl.MEM.CPU);\n",
    "    if (error_code != sl.ERROR_CODE.SUCCESS):\n",
    "        raise SystemExit(f\"Failed to retrieve color image: {error_code}\");\n",
    "\n",
    "    # retrieve current depth map\n",
    "    error_code = camera.retrieve_measure(depth_image, sl.MEASURE.DEPTH, sl.MEM.CPU);\n",
    "    if (error_code != sl.ERROR_CODE.SUCCESS):\n",
    "        raise SystemExit(f\"Failed to retrieve depth image: {error_code}\");\n",
    "\n",
    "    # resize and convert image for model\n",
    "    sized_image: np.ndarray = resize_image(color_image.get_data());\n",
    "    sized_image = cv.cvtColor(sized_image, cv.COLOR_BGRA2RGB);\n",
    "\n",
    "    # resize and normalize depth for comparison\n",
    "    sized_depth: np.ndarray = resize_image(depth_image.get_data());\n",
    "    # sized_depth = (255 * sized_depth / MAX_DEPTH).astype(np.uint8);\n",
    "\n",
    "    # prepare image as torch.Tensor\n",
    "    img_tensor = ToTensor()(sized_image)\n",
    "    img_tensor = img_tensor[None, :, :, :]\n",
    "    img_tensor = img_tensor.to(device);\n",
    "\n",
    "    # let the model create a depth map from the frame\n",
    "    with torch.no_grad():\n",
    "        prediction = model(img_tensor);\n",
    "        pred_tensor: torch.Tensor = prediction[\"pred_d\"]\n",
    "\n",
    "    # convert tensor back to numpy array\n",
    "    pred_ndarray: np.ndarray = pred_tensor.squeeze().cpu().numpy()\n",
    "    # pred_ndarray = (pred_ndarray / MAX_DEPTH) * 255\n",
    "    # pred_ndarray = pred_ndarray.astype(np.uint8)\n",
    "\n",
    "    # calculate PSNR\n",
    "    psnr = PSNR(sized_depth, pred_ndarray);\n",
    "    current_frame = camera.get_svo_position() + 1;\n",
    "\n",
    "    output.append(f\"{current_frame},{color_image.timestamp.data_ns},{psnr}\\n\");\n",
    "\n",
    "    print(f\"[{current_frame}/{nr_frames}] PSNR = {psnr} dB\", end='\\r');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(SVO_FILEPATH.replace(\".svo\", '') + '-' + OUTPUT_FILE, 'w') as file:\n",
    "    file.write(\"Frame,Timestamp [ns],PSNR [dB]\\n\")\n",
    "    file.writelines(output);"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
