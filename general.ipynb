{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Monocular Depth Estimation on Artificially Scaled Up Camera Images for Robot Applications\n",
    "\n",
    "This notebook will estimate the performance of monocular depth sensing on zoomed in targets in a robotics application."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#std libs\n",
    "from collections import OrderedDict\n",
    "import math\n",
    "import os\n",
    "import typing\n",
    "import time\n",
    "\n",
    "#non std libs\n",
    "import torch\n",
    "from torchvision.transforms import ToTensor\n",
    "import cv2 as cv\n",
    "import numpy as np\n",
    "from pyzed import sl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/Daten2/Uni/Projekt_BCV/.venv/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from monocular_depth_estimation.models.model import GLPDepth\n",
    "\n",
    "from super_resolution.models.RRDBNet_arch import RRDBNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters\n",
    "\n",
    "Name            | Type   | Value\n",
    "----------------|--------|----------------------------------------------------------------------------------------------------------------------------------------------\n",
    "`SVO_FILEPATH`  | `str`  | Path to the svo recording used for evaluation.\n",
    "`USE_NYU`       | `bool` | `True`: Use the weights pretrained on \"nyudepthv2\"<br>`False`: Use the weights pretrained on the kitti eigen split\n",
    "`CROP_TO_SIZE`  | `bool` | `True`: Crop the image to size (1216, 352). This loses parts of the image completely.<br>`False`: Resize the image. This distortes the image.\n",
    "`OUTPUT_SUFFIX` | `str`  | Suffix of the output csv file for evaluation results. The final file name will consist of the svo filename, NYU/KITTI, CROP/RESIZE and this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "SVO_FILEPATH    = \"/mnt/Daten2/Uni/HiWi_Husky/recordings/test_recording_rear.svo\";\n",
    "USE_NYU         = False;\n",
    "CROP_TO_SIZE    = True;\n",
    "OUTPUT_SUFFIX   = \"general\";"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_DEPTH = 20.0 #[m]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definition utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PSNR(ground_truth: np.ndarray, estimation: np.ndarray) -> typing.Tuple[float, float]:\n",
    "    mse = np.nanmean((ground_truth - estimation) ** 2);\n",
    "    if (mse == 0): #no noise\n",
    "        return -1.0, mse;\n",
    "\n",
    "    psnr = 20 * math.log10(255.0) - 10 * math.log10(mse);\n",
    "    return psnr, mse;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_image(img: np.ndarray) -> np.ndarray:\n",
    "    h_im, w_im = img.shape[:2]\n",
    "\n",
    "    margin_top = int(h_im - 352)\n",
    "    margin_left = int((w_im - 1216) / 2)\n",
    "\n",
    "    sized_image = img[margin_top:  margin_top  + 352,\n",
    "                      margin_left: margin_left + 1216]\n",
    "\n",
    "    return sized_image;\n",
    "\n",
    "if CROP_TO_SIZE:\n",
    "    resize_image = crop_image;\n",
    "else:\n",
    "    resize_image = lambda img: cv.resize(img, (1216, 352));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialization of camera object for svo playback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "camera_init_parameters = sl.InitParameters();\n",
    "\n",
    "camera_init_parameters.svo_real_time_mode = False;\n",
    "camera_init_parameters.open_timeout_sec = 30;\n",
    "camera_init_parameters.coordinate_units = sl.UNIT.METER;\n",
    "\n",
    "camera_init_parameters.set_from_svo_file(SVO_FILEPATH);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "camera = sl.Camera();\n",
    "error_code = camera.open(camera_init_parameters);\n",
    "if (error_code != sl.ERROR_CODE.SUCCESS):\n",
    "    print(\"Failed to open Camera object:\", error_code);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "nr_frames = camera.get_svo_number_of_frames();\n",
    "resolution = camera.get_camera_information().camera_configuration.camera_resolution;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "color_image = sl.Mat(resolution.width, resolution.height, sl.MAT_TYPE.U8_C3, sl.MEM.CPU);\n",
    "depth_image = sl.Mat(resolution.width, resolution.height, sl.MAT_TYPE.F32_C1, sl.MEM.CPU);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup of pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU: NVIDIA GeForce RTX 2070 7.79GiB (CC: 7.5)\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\");\n",
    "    device_prop = torch.cuda.get_device_properties(device);\n",
    "    print(f\"Using GPU: {device_prop.name} {round(device_prop.total_memory / 1024**3, 2)}GiB (CC: {device_prop.major}.{device_prop.minor})\");\n",
    "else:\n",
    "    device = torch.device(\"cpu\");\n",
    "    print(\"Using CPU.\");\n",
    "torch.set_default_device(device);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup of monocular depth estimation model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Storage:\n",
    "    #a minimal storage class to somewhat mimic the behaviour of argparse.ArgumentParser\n",
    "    def __init__(self, **kwargs):\n",
    "        self.__dict__.update(kwargs);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### KITTI parameters\n",
    "\n",
    "```bash\n",
    "python3 test.py \\\n",
    "--dataset kitti \\\n",
    "--kitti_crop garg_crop \\\n",
    "--data_path ../data/ \\\n",
    "--max_depth 80.0 \\\n",
    "--max_depth_eval 80.0 \\\n",
    "--backbone swin_large_v2 \\\n",
    "--depths 2 2 18 2 \\\n",
    "--num_filters 32 32 32 \\\n",
    "--deconv_kernels 2 2 2 \\\n",
    "--window_size 22 22 22 11 \\\n",
    "--pretrain_window_size 12 12 12 6 \\\n",
    "--use_shift True True False False \\\n",
    "--flip_test \\\n",
    "--shift_window_test \\\n",
    "--shift_size 16 \\\n",
    "--do_evaluate \\\n",
    "--ckpt_dir ckpt/kitti_swin_large.ckpt\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "kitti_args = Storage(\n",
    "    #max_depth=80.0,\n",
    "    backbone=\"swin_large_v2\",\n",
    "    depths=[2, 2, 18, 2],\n",
    "    window_size=[22, 22, 22, 11],\n",
    "    pretrain_window_size=[12, 12, 12, 6],\n",
    "    drop_path_rate=0.3,\n",
    "    use_checkpoint=False,\n",
    "    use_shift=[True, True, False, False],\n",
    "    pretrained='',\n",
    "    num_deconv=3,\n",
    "    num_filters=[32, 32, 32],\n",
    "    deconv_kernels=[2, 2, 2],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### nyudepth parameters\n",
    "\n",
    "```bash\n",
    "python3 test.py \\\n",
    "--dataset nyudepthv2 \\\n",
    "--data_path ../data/ \\\n",
    "--max_depth 10.0 \\\n",
    "--max_depth_eval 10.0  \\\n",
    "--backbone swin_large_v2 \\\n",
    "--depths 2 2 18 2 \\\n",
    "--num_filters 32 32 32 \\\n",
    "--deconv_kernels 2 2 2 \\\n",
    "--window_size 30 30 30 15 \\\n",
    "--pretrain_window_size 12 12 12 6 \\\n",
    "--use_shift True True False False \\\n",
    "--flip_test \\\n",
    "--shift_window_test \\\n",
    "--shift_size 2 \\\n",
    "--do_evaluate \\\n",
    "--ckpt_dir ckpt/nyudepthv2_swin_large.ckpt\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "nyudepthv2_args = Storage(\n",
    "    #max_depth=10.0,\n",
    "    backbone=\"swin_large_v2\",\n",
    "    depths=[2, 2, 18, 2],\n",
    "    window_size=[30, 30, 30, 15],\n",
    "    pretrain_window_size=[12, 12, 12, 6],\n",
    "    drop_path_rate=0.3, #\n",
    "    use_checkpoint=False, #\n",
    "    use_shift=[True, True, False, False],\n",
    "    pretrained='', #\n",
    "    num_deconv=3, #\n",
    "    num_filters=[32, 32, 32],\n",
    "    deconv_kernels=[2, 2, 2],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# as specified for the svo recording\n",
    "nyudepthv2_args.max_depth = kitti_args.max_depth = 20.0;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using weights pretrained on kitti.\n",
      "norm8_log_bylayer: [(22, 22)] ==> [12]\n",
      "norm8_log_bylayer: [(22, 22)] ==> [12]\n",
      "norm8_log_bylayer: [(22, 22)] ==> [12]\n",
      "norm8_log_bylayer: [(22, 22)] ==> [12]\n",
      "norm8_log_bylayer: [(22, 22)] ==> [12]\n",
      "norm8_log_bylayer: [(22, 22)] ==> [12]\n",
      "norm8_log_bylayer: [(22, 22)] ==> [12]\n",
      "norm8_log_bylayer: [(22, 22)] ==> [12]\n",
      "norm8_log_bylayer: [(22, 22)] ==> [12]\n",
      "norm8_log_bylayer: [(22, 22)] ==> [12]\n",
      "norm8_log_bylayer: [(22, 22)] ==> [12]\n",
      "norm8_log_bylayer: [(22, 22)] ==> [12]\n",
      "norm8_log_bylayer: [(22, 22)] ==> [12]\n",
      "norm8_log_bylayer: [(22, 22)] ==> [12]\n",
      "norm8_log_bylayer: [(22, 22)] ==> [12]\n",
      "norm8_log_bylayer: [(22, 22)] ==> [12]\n",
      "norm8_log_bylayer: [(22, 22)] ==> [12]\n",
      "norm8_log_bylayer: [(22, 22)] ==> [12]\n",
      "norm8_log_bylayer: [(22, 22)] ==> [12]\n",
      "norm8_log_bylayer: [(22, 22)] ==> [12]\n",
      "norm8_log_bylayer: [(22, 22)] ==> [12]\n",
      "norm8_log_bylayer: [(22, 22)] ==> [12]\n",
      "norm8_log_bylayer: [(11, 11)] ==> [6]\n",
      "norm8_log_bylayer: [(11, 11)] ==> [6]\n"
     ]
    }
   ],
   "source": [
    "if USE_NYU:\n",
    "    print(\"Using weights pretrained on nyudepthv2.\")\n",
    "    mde_model = GLPDepth(args=nyudepthv2_args);\n",
    "\n",
    "    mde_model_weights: dict = torch.load(\"monocular_depth_estimation/checkpoints/nyudepthv2_swin_large.ckpt\", map_location=device);\n",
    "else:\n",
    "    print(\"Using weights pretrained on kitti.\")\n",
    "    mde_model = GLPDepth(args=kitti_args);\n",
    "\n",
    "    mde_model_weights: dict = torch.load(\"monocular_depth_estimation/checkpoints/kitti_swin_large.ckpt\", map_location=device);\n",
    "\n",
    "mde_model = mde_model.to(device=device);\n",
    "\n",
    "if 'module' in next(iter(mde_model_weights.items()))[0]:\n",
    "    model_weight = OrderedDict((k[7:], v) for k, v in mde_model_weights.items())\n",
    "\n",
    "mde_model.load_state_dict(mde_model_weights);\n",
    "mde_model.eval();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup of super resolution model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "sr_model = RRDBNet(3, 3, 64, 23, gc=32);\n",
    "sr_model.to(device=device);\n",
    "\n",
    "sr_model_weights: dict = torch.load(\"super_resolution/models/RRDB_ESRGAN_x4.pth\", map_location=device);\n",
    "\n",
    "sr_model.load_state_dict(sr_model_weights);\n",
    "sr_model.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done                                                            \n"
     ]
    }
   ],
   "source": [
    "output = np.ndarray((nr_frames, 4), np.float64);\n",
    "\n",
    "camera.set_svo_position(0);\n",
    "while (True):\n",
    "    # get/go to the current frame\n",
    "    error_code = camera.grab();\n",
    "    if (error_code == sl.ERROR_CODE.END_OF_SVOFILE_REACHED):\n",
    "        print(\"Done\" + ' ' * 60)\n",
    "        break\n",
    "    elif (error_code != sl.ERROR_CODE.SUCCESS):\n",
    "        raise SystemExit(f\"Failed to grab frame: {error_code}\");\n",
    "\n",
    "    # retrieve current camera frame\n",
    "    error_code = camera.retrieve_image(color_image, sl.VIEW.LEFT, sl.MEM.CPU);\n",
    "    if (error_code != sl.ERROR_CODE.SUCCESS):\n",
    "        raise SystemExit(f\"Failed to retrieve color image: {error_code}\");\n",
    "\n",
    "    # retrieve current depth map\n",
    "    error_code = camera.retrieve_measure(depth_image, sl.MEASURE.DEPTH, sl.MEM.CPU);\n",
    "    if (error_code != sl.ERROR_CODE.SUCCESS):\n",
    "        raise SystemExit(f\"Failed to retrieve depth image: {error_code}\");\n",
    "\n",
    "    # get processing start time\n",
    "    start_time = time.time();\n",
    "\n",
    "    # resize and convert image for model\n",
    "    sized_image: np.ndarray = resize_image(color_image.get_data());\n",
    "    sized_image = cv.cvtColor(sized_image, cv.COLOR_BGRA2RGB);\n",
    "\n",
    "    downsized_image = cv.resize(sized_image, (304, 88)) # (304, 88) is exactly 1/4 of the original size\n",
    "\n",
    "    # resize and normalize depth for comparison\n",
    "    sized_depth: np.ndarray = resize_image(depth_image.get_data());\n",
    "\n",
    "    # prepare downsized image for super resolution model\n",
    "    downsized_img_tensor = ToTensor()(downsized_image);\n",
    "    downsized_img_tensor = downsized_img_tensor.unsqueeze(0).to(device=device);\n",
    "\n",
    "    # let the super resolution modek scake up the image\n",
    "    with torch.no_grad():\n",
    "        prediction = sr_model(downsized_img_tensor);\n",
    "        pred_tensor = prediction.data;\n",
    "\n",
    "    # prepare upsized image for depth estimation model\n",
    "    img_tensor = pred_tensor.to(dtype=torch.float32).clamp_(0, 1);\n",
    "\n",
    "    # let the depth estimation model create a depth map from the frame\n",
    "    with torch.no_grad():\n",
    "        prediction = mde_model(img_tensor);\n",
    "        pred_tensor: torch.Tensor = prediction[\"pred_d\"]\n",
    "\n",
    "    # convert tensor back to numpy array\n",
    "    depth_prediction: np.ndarray = pred_tensor.squeeze().to(device=torch.device(\"cpu\")).numpy()\n",
    "\n",
    "    # get processing end time\n",
    "    end_time = time.time();\n",
    "\n",
    "    # cleanup data -> trat inf as nan\n",
    "    infmask = np.where(np.isinf(depth_prediction));\n",
    "    depth_prediction[infmask] = np.nan;\n",
    "    # print(\"estimation infmask: \", infmask);\n",
    "\n",
    "    infmask = np.where(np.isinf(sized_depth));\n",
    "    sized_depth[infmask] = np.nan;\n",
    "    # print(\"ground_truth infmask: \", infmask);\n",
    "\n",
    "    # debug\n",
    "    # print(\"gt:\", np.nanmin(sized_depth), np.nanmax(sized_depth));\n",
    "    # print(\"pred:\", np.nanmin(depth_prediction), np.nanmax(depth_prediction));\n",
    "\n",
    "    # calculate PSNR\n",
    "    psnr, mse = PSNR(sized_depth, depth_prediction);\n",
    "    current_frame = camera.get_svo_position();\n",
    "\n",
    "    # output.append(f\"{current_frame},{color_image.timestamp.data_ns},{psnr}\\n\");\n",
    "    output[current_frame] = [color_image.timestamp.get_milliseconds(), end_time - start_time, psnr, mse];\n",
    "\n",
    "    print(f\"[{current_frame + 1}/{nr_frames}]\", f\"PSNR = {round(psnr, 4)} dB\", f\"MSE = {round(mse, 4)}\", sep='\\t', end='\\r');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make the timestamps start at 0s\n",
    "output[:, 0] = (output[:, 0] - output[0, 0]) / 1000;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing output to: test_recording_rear-KITTI-CROPPED-output.csv\n"
     ]
    }
   ],
   "source": [
    "output_file_name = '-'.join([\n",
    "    os.path.basename(SVO_FILEPATH).replace(\".svo\", ''),\n",
    "    (\"NYU\" if USE_NYU else \"KITTI\"),\n",
    "    (\"CROPPED\" if CROP_TO_SIZE else \"RESIZED\"),\n",
    "    OUTPUT_SUFFIX\n",
    "]) + \".csv\";\n",
    "print(\"Writing output to:\", output_file_name);\n",
    "\n",
    "with open(output_file_name, 'w') as file:\n",
    "    file.write(\"Frame;Timestamp [s];Processingtime [s];PSNR [dB];MSE\\n\")\n",
    "\n",
    "    for i, (timestamp, timedelta, psnr, mse) in enumerate(output):\n",
    "        file.write(f\"{i};{timestamp};{timedelta};{psnr};{mse}\\n\");\n",
    "\n",
    "    file.write(f\"Mean:;;{np.mean(output[:, 1])};{np.mean(output[:, 2])};{np.mean(output[:, 3])}\\n\");\n",
    "    file.write(f\"Median:;;{np.median(output[:, 1])};{np.median(output[:, 2])};{np.median(output[:, 3])}\\n\");"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
